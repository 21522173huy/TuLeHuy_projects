{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:36:25.915246Z","iopub.status.busy":"2024-05-07T10:36:25.914857Z","iopub.status.idle":"2024-05-07T10:38:16.866622Z","shell.execute_reply":"2024-05-07T10:38:16.865316Z","shell.execute_reply.started":"2024-05-07T10:36:25.915216Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\n","Collecting transformers\n","  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.20,>=0.19 (from transformers)\n","  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.2\n","    Uninstalling tokenizers-0.15.2:\n","      Successfully uninstalled tokenizers-0.15.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.39.3\n","    Uninstalling transformers-4.39.3:\n","      Successfully uninstalled transformers-4.39.3\n","Successfully installed tokenizers-0.19.1 transformers-4.40.2\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n","Collecting nltk\n","  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\n","Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: nltk\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.4\n","    Uninstalling nltk-3.2.4:\n","      Successfully uninstalled nltk-3.2.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nltk-3.8.1\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.1.2)\n","Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.1.4)\n","Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.40.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.26.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.31.0)\n","Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.1)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.5)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert-score) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.2.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.22.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: bert-score\n","Successfully installed bert-score-0.3.13\n"]}],"source":["!pip install --upgrade transformers\n","!pip install --upgrade nltk\n","!pip install -q evaluate\n","!pip install bert-score\n","!pip install -q rouge_score\n","!pip install -q git+https://github.com/salaniz/pycocoevalcap.git"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:38:16.868982Z","iopub.status.busy":"2024-05-07T10:38:16.868670Z","iopub.status.idle":"2024-05-07T10:38:17.897667Z","shell.execute_reply":"2024-05-07T10:38:17.896624Z","shell.execute_reply.started":"2024-05-07T10:38:16.868950Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'3.8.1'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.__version__"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:38:17.899301Z","iopub.status.busy":"2024-05-07T10:38:17.898975Z","iopub.status.idle":"2024-05-07T10:38:21.808158Z","shell.execute_reply":"2024-05-07T10:38:21.807178Z","shell.execute_reply.started":"2024-05-07T10:38:17.899272Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","# device will determine whether to run the training on GPU or CPU.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:38:21.811866Z","iopub.status.busy":"2024-05-07T10:38:21.811247Z","iopub.status.idle":"2024-05-07T10:38:21.841062Z","shell.execute_reply":"2024-05-07T10:38:21.840285Z","shell.execute_reply.started":"2024-05-07T10:38:21.811828Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.device_count()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:38:21.842239Z","iopub.status.busy":"2024-05-07T10:38:21.841991Z","iopub.status.idle":"2024-05-07T10:38:21.847271Z","shell.execute_reply":"2024-05-07T10:38:21.846299Z","shell.execute_reply.started":"2024-05-07T10:38:21.842217Z"},"trusted":true},"outputs":[],"source":["def count_params(model):\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:38:21.848753Z","iopub.status.busy":"2024-05-07T10:38:21.848410Z","iopub.status.idle":"2024-05-07T10:38:37.757442Z","shell.execute_reply":"2024-05-07T10:38:37.756675Z","shell.execute_reply.started":"2024-05-07T10:38:21.848728Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-07 10:38:26.948360: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-07 10:38:26.948495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-07 10:38:27.078551: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"272cea7d5c014a16b13b03c1448b9c61","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/276 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"966c87a868124f25b2342455c917dca4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6d2f3b04b8746de86b148fb6c0484e4","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71d40d7fe5b24357bc00f73c0e5ef8a0","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e869fadaa5be4cb09567fca7ccfa2bfa","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration, BeitImageProcessor, AutoModel\n","\n","# Vision\n","vision_module = 'microsoft/beit-base-patch16-224-pt22k-ft22k'\n","feature_extractor = BeitImageProcessor.from_pretrained(vision_module)\n","\n","# Language\n","language_module = 'luqh/ClinicalT5-base'\n","tokenizer = T5Tokenizer.from_pretrained(language_module)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:38:37.759249Z","iopub.status.busy":"2024-05-07T10:38:37.758765Z","iopub.status.idle":"2024-05-07T10:38:38.082953Z","shell.execute_reply":"2024-05-07T10:38:38.082195Z","shell.execute_reply.started":"2024-05-07T10:38:37.759224Z"},"trusted":true},"outputs":[],"source":["import glob\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","import torchvision.transforms as transforms\n","import pickle\n","\n","class ImageCLEF(Dataset):\n","  def __init__(self, tokenizer = None, feature_extractor = None, image_folder = None, data_csv_path = None, one_hot_path = None, cuis_csv_path = None, data=None):\n","\n","    self.tokenizer = tokenizer\n","    self.feature_extractor = feature_extractor\n","    # if data is None all others argument cant be None\n","    assert (tokenizer is not None\n","            and feature_extractor is not None\n","            and image_folder is not None\n","            and data_csv_path is not None\n","            and one_hot_path is not None\n","            and cuis_csv_path is not None) or data is not None, \"All other arguments must be passed if data is None!\"\n","\n","    if data is None:\n","      self.load_data(image_folder, data_csv_path, one_hot_path, cuis_csv_path)\n","    else:\n","      self.data = data\n","\n","  def load_data(self, image_folder, data_csv_path, one_hot_path, cuis_csv_path):\n","    self.cuis_mapping = pd.read_csv(cuis_csv_path)\n","\n","    with open(one_hot_path, 'rb') as file:\n","      one_hot = pickle.load(file)\n","\n","    self.one_hot = one_hot\n","    self.data = []\n","\n","    data_csv = pd.read_csv(data_csv_path)\n","\n","    # Setup the total feature file\n","    image_paths = glob.glob(image_folder + '/*')\n","\n","    for path in tqdm(image_paths):\n","\n","    # Obtaining Image Id\n","      image_id = path.split('/')[-1].split('.')[0]\n","\n","      # Mapping image id with other variables\n","      # caption = data_csv[data_csv['image_id'] == image_id]['caption'].item()\n","      caption = data_csv[data_csv['ID'] == image_id]['Caption'].item()\n","\n","      sample = {\n","          'image_id': image_id,\n","          'path' : path,\n","          'captions': caption,\n","      }\n","\n","      self.data.append(sample)\n","\n","  def __getitem__(self, idx):\n","    sample = self.data[idx]\n","\n","    return {\n","      'image_id': sample['image_id'],\n","      'path' : sample['path'],\n","      'captions': sample['captions'],\n","    }\n","\n","  def split_data(self, validation_size, random_state=42):\n","\n","    # Split train and evaluation set\n","    train_data, val_data = train_test_split(self.data,\n","                                                 test_size=validation_size,\n","                                                 random_state=random_state)\n","\n","    return (ImageCLEF(tokenizer=self.tokenizer, data=train_data),\n","            ImageCLEF(tokenizer=self.tokenizer, data=val_data))\n","\n","  def num_classes(self):\n","    return len(self.one_hot.classes_)\n","\n","  def get_classes(self):\n","    return self.one_hot.classes_\n","\n","  def get_id_name(self, binary_values):\n","\n","    mapped_values, mapped_name = [], []\n","\n","    for i, binary in enumerate(eval(binary_values)):\n","      if binary == 1:\n","          res = self.one_hot.classes_[i]\n","          mapped_values.append(res)\n","\n","          name = self.cuis_mapping[self.cuis_mapping['CUI'] == res]['Canonical name'].item()\n","          mapped_name.append(name)\n","\n","    return mapped_values, mapped_name\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def collate_fn(self, batch):\n","\n","    images = [Image.open(each['path']).convert('RGB') for each in batch]\n","    # label_encode = [each['label_encode'] for each in batch]\n","\n","    raw_captions = [each['captions'] for each in batch]\n","    image_ids = [each['image_id'] for each in batch]\n","\n","    extracted_images = self.feature_extractor(images = images, return_tensors = 'pt')\n","    tokenized_captions = self.tokenizer(raw_captions, padding = True, truncation = True, max_length = 128, return_tensors = 'pt')\n","    # label_encode = pad_sequence(self.transform_tensor(label_encode), batch_first = True, padding_value = self.num_classes())\n","\n","    sample = {\n","      'ids' : image_ids,\n","      'raw_captions' : raw_captions,\n","      'pixel_values' : extracted_images.pixel_values, # tensor\n","      'labels' : tokenized_captions.input_ids, # tensor\n","      'attention_mask' : tokenized_captions.attention_mask\n","    }\n","\n","    return sample"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:38:38.084484Z","iopub.status.busy":"2024-05-07T10:38:38.084131Z","iopub.status.idle":"2024-05-07T10:39:02.343655Z","shell.execute_reply":"2024-05-07T10:39:02.342505Z","shell.execute_reply.started":"2024-05-07T10:38:38.084450Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0511ad0cdf494e129f6b5ee8856be91e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9972 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["data = ImageCLEF(tokenizer = tokenizer,\n","                feature_extractor = feature_extractor,\n","                image_folder ='/kaggle/input/testlehuytu2024/valid_images/valid',\n","                data_csv_path = '/kaggle/input/testlehuytu2024/valid_captions.csv',\n","                one_hot_path = '/kaggle/input/testlehuytu2024/one_hot (1).pkl',\n","                cuis_csv_path = '/kaggle/input/testlehuytu2024/cui_mapping.csv',\n","                )"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:39:02.346061Z","iopub.status.busy":"2024-05-07T10:39:02.345081Z","iopub.status.idle":"2024-05-07T10:39:02.351492Z","shell.execute_reply":"2024-05-07T10:39:02.350615Z","shell.execute_reply.started":"2024-05-07T10:39:02.346021Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","dataloader = DataLoader(data, batch_size=1, shuffle=False, collate_fn=data.collate_fn)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:39:02.356936Z","iopub.status.busy":"2024-05-07T10:39:02.356390Z","iopub.status.idle":"2024-05-07T10:39:02.373722Z","shell.execute_reply":"2024-05-07T10:39:02.372726Z","shell.execute_reply.started":"2024-05-07T10:39:02.356894Z"},"trusted":true},"outputs":[{"data":{"text/plain":["9972"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from Captioning_Module import BeIT_T5_Concepts\n","from Concepts_Module import ConceptDetectionModel"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:39:02.412299Z","iopub.status.busy":"2024-05-07T10:39:02.411942Z","iopub.status.idle":"2024-05-07T10:39:21.408993Z","shell.execute_reply":"2024-05-07T10:39:21.408156Z","shell.execute_reply.started":"2024-05-07T10:39:02.412276Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"233f542b41fd42fa9ab05d0fdcf2e874","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad8fc91b48ea4db7817a129e960acac1","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/414M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d6dea79219049f7af529be7696e93cb","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b925380ebf554c40ad62d5711ce02bc6","version_major":2,"version_minor":0},"text/plain":["flax_model.msgpack:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/modeling_flax_pytorch_utils.py:460: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n","  pt_model_dict[flax_key] = torch.from_numpy(flax_tensor)\n","All Flax model weights were used when initializing T5ForConditionalGeneration.\n","\n","Some weights of T5ForConditionalGeneration were not initialized from the Flax model and are newly initialized: ['lm_head.weight', 'encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_concepts = ConceptDetectionModel(pretrained_image_name = vision_module,\n","                                       num_labels = data.num_classes()\n",")\n","\n","model_caption = nn.DataParallel(BeIT_T5_Concepts(vision_module = vision_module,\n","                         language_module = language_module,\n","                         device= device,\n","))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:39:21.410439Z","iopub.status.busy":"2024-05-07T10:39:21.410160Z","iopub.status.idle":"2024-05-07T10:39:32.214631Z","shell.execute_reply":"2024-05-07T10:39:32.213528Z","shell.execute_reply.started":"2024-05-07T10:39:21.410415Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from safetensors.torch import load\n","\n","file_path = \"/kaggle/input/testlehuytu2024/model_concepts_checkpoint.safetensors\"\n","with open(file_path, \"rb\") as f:\n","    concepts_weight = f.read()\n","\n","model_concepts.load_state_dict(load(concepts_weight))\n","\n","model_caption.module.load_state_dict(torch.load('/kaggle/input/testlehuytu2024/State_dict_BeIT5_Epoch_1_4100.pth', map_location = torch.device(device)))"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:39:32.216432Z","iopub.status.busy":"2024-05-07T10:39:32.216030Z","iopub.status.idle":"2024-05-07T10:39:32.225266Z","shell.execute_reply":"2024-05-07T10:39:32.224392Z","shell.execute_reply.started":"2024-05-07T10:39:32.216394Z"},"trusted":true},"outputs":[],"source":["from pycocoevalcap.cider.cider import Cider\n","cider = Cider()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:39:32.227335Z","iopub.status.busy":"2024-05-07T10:39:32.226644Z","iopub.status.idle":"2024-05-07T10:39:35.539073Z","shell.execute_reply":"2024-05-07T10:39:35.538303Z","shell.execute_reply.started":"2024-05-07T10:39:32.227299Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e66f71c9120d4730ba35c7788886f6aa","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.93k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"552fa416e0954b88be659fff606aa61f","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76e011b87ee14b8ba2d62e382a349eb2","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de959606f772406cbcd01836b7203b73","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e02d10ff8fed48e9af579abc51204141","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d7d59cbcf014c3aa047339f927f72a2","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import evaluate\n","import numpy as np\n","\n","meteor = evaluate.load('meteor')\n","rouge = evaluate.load('rouge')\n","bleu = evaluate.load(\"bleu\")\n","bertscore = evaluate.load(\"bertscore\")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:45:05.840120Z","iopub.status.busy":"2024-05-07T10:45:05.839451Z","iopub.status.idle":"2024-05-07T10:45:05.850680Z","shell.execute_reply":"2024-05-07T10:45:05.849592Z","shell.execute_reply.started":"2024-05-07T10:45:05.840089Z"},"trusted":true},"outputs":[],"source":["def calculate_mean(numbers):\n","    total = sum(numbers)\n","    count = len(numbers)\n","    mean = total / count\n","    return mean\n","\n","def compute_metrics(pred_ans, ground_t):\n","\n","  print(f'Prediction : {pred_ans}')\n","  print(f'Ground_truth : {ground_t}')\n","  print('       ')\n","\n","  # Compute BLEU, ROUGE, METEOR\n","  bleu1_score = bleu.compute(predictions=pred_ans, references=ground_t, max_order=1)['bleu']\n","  bleu2_score = bleu.compute(predictions=pred_ans, references=ground_t, max_order=2)['bleu']\n","  bleu3_score = bleu.compute(predictions=pred_ans, references=ground_t, max_order=3)['bleu']\n","  bleu4_score = bleu.compute(predictions=pred_ans, references=ground_t, max_order=4)['bleu']\n","  rouge_score = rouge.compute(predictions=pred_ans, references=ground_t)['rougeL']\n","  meteor_score = meteor.compute(predictions=pred_ans, references=ground_t)['meteor']\n","\n","  # Compute CIDER score\n","  hypotheses_dict = {i: [h] for i, h in enumerate(pred_ans)}\n","  references_dict = {i: [r] for i, r in enumerate(ground_t)}\n","\n","  cider_score, _ = cider.compute_score(references_dict, hypotheses_dict)\n","\n","  # Compute Bert Score\n","  bert_score = bertscore.compute(predictions=pred_ans, references=ground_t, model_type = 'microsoft/deberta-xlarge-mnli', device = device)\n","  bert_score_F1_mean = calculate_mean(bert_score['f1'])\n","\n","  return np.array([bert_score_F1_mean, bleu1_score, bleu2_score, bleu3_score, bleu4_score, rouge_score, meteor_score, cider_score])"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:45:10.592492Z","iopub.status.busy":"2024-05-07T10:45:10.591891Z","iopub.status.idle":"2024-05-07T10:45:10.606337Z","shell.execute_reply":"2024-05-07T10:45:10.605255Z","shell.execute_reply.started":"2024-05-07T10:45:10.592456Z"},"trusted":true},"outputs":[],"source":["def get_answer(preds):\n","    p = np.where(np.array(preds)>=0.5,1,0)\n","    return p\n","\n","def predict_caption(model_caption, model_concepts, pixel_value, tokenizer, convert_logits_concepts, device, max_length = 50):\n","\n","  pixel_value = pixel_value.to(device)\n","\n","  # GENERATE CONCEPTS (Follow Model Concepts Forward Function)\n","  encoded_image = model_concepts.image_encoder(pixel_values = pixel_value,\n","                                                 return_dict = True,)\n","\n","  encoded_image = encoded_image['pooler_output'].squeeze()\n","  output = model_concepts.linear(encoded_image) #batch_size X dim\n","\n","  # Concepts Logits\n","  logits = model_concepts.classifier(output)\n","  preds = torch.sigmoid(torch.Tensor(logits)).detach().cpu().numpy()\n","  preds = get_answer(preds)\n","\n","  # Convert Logits To Concepts\n","  mapped_names = convert_logits_concepts(str(preds.tolist()))[1]\n","  mapped_names = ', '.join(mapped_names)\n","\n","  print(f'Predicted Concepts: {mapped_names}')\n","\n","  # GENERATE CAPTION (Follow Model Caption Forward Function)\n","\n","  # Encoding Image\n","  image_features = model_caption.image_encoder(pixel_values = pixel_value).last_hidden_state\n","  image_features_ = model_caption.layer_norm(image_features)\n","\n","  # Encoding Concepts\n","  tokenized_name = tokenizer(mapped_names, padding = True, truncation = True, max_length = 128, return_tensors = 'pt')\n","\n","  concept_features = model_caption.concepts_encoder(\n","      input_ids = tokenized_name.input_ids.to(device),\n","      attention_mask = tokenized_name.attention_mask.to(device)\n","  ).last_hidden_state # batch_size, num_concepts, 768\n","\n","  # Concat Image and Concepts Features\n","  cross_features = model_caption.layer_norm(torch.cat([image_features_, concept_features], dim = 1)) # batch_size, 193 + num_concepts, 768\n","  cross_mask = torch.cat([torch.ones(image_features_.size(0), image_features_.size(1)), tokenized_name.attention_mask], dim = 1) # batch_size, 193 + num_concepts\n","\n","  # Generate Answer\n","  count = 0\n","\n","  input_ids = torch.tensor([model_caption.captions_decoder.config.decoder_start_token_id]).to(device) # Decoder Start Token\n","\n","  while count <= max_length:\n","\n","    decoded_tokens = model_caption.captions_decoder(input_ids = input_ids.unsqueeze(dim = 0),\n","                        encoder_hidden_states = cross_features,\n","                        encoder_attention_mask = cross_mask.to(device)\n","                        ).last_hidden_state # batch_size, answer_length, 768\n","\n","    output_tokens = model_caption.lm_head(decoded_tokens) # batch_size, answer_length, vocab_size\n","\n","    tokens_ids = output_tokens.argmax(dim = -1) # batch_size, answer_length\n","\n","    # if tokens_ids[:, -1] == tokenizer.eos_token_id :\n","    #   print(f'Found Eos Token in length : {count}')\n","    #   break\n","\n","    input_ids = torch.cat([input_ids, tokens_ids[:, -1]], dim = -1)\n","\n","    count += 1\n","\n","  res = tokenizer.decode(input_ids, skip_special_tokens = True)\n","  return res"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:45:10.742400Z","iopub.status.busy":"2024-05-07T10:45:10.741848Z","iopub.status.idle":"2024-05-07T10:45:10.752350Z","shell.execute_reply":"2024-05-07T10:45:10.751433Z","shell.execute_reply.started":"2024-05-07T10:45:10.742370Z"},"trusted":true},"outputs":[],"source":["def predict(model_caption, model_concepts, dataloader, tokenizer, convert_logits_concepts, device = 'cpu'):\n","\n","  res_ids, res_predictions, res_captions = [], [], []\n","  total_scores = 0\n","\n","  model_caption.to(device), model_concepts.to(device)\n","  model_caption.eval(), model_concepts.eval()\n","\n","  with torch.no_grad():\n","\n","    for i, sample in enumerate(tqdm(dataloader)):\n","\n","      pixel_value = sample['pixel_values']\n","      image_id = sample['ids']\n","\n","      predicted_captions = predict_caption(model_caption = model_caption.module,\n","                                           model_concepts = model_concepts,\n","                                           pixel_value = pixel_value,\n","                                           tokenizer = tokenizer,\n","                                           convert_logits_concepts = convert_logits_concepts,\n","                                           device = device)\n","      total_scores += compute_metrics([predicted_captions], sample['raw_captions'])\n","        \n","      res_ids.append(image_id[0])\n","      res_predictions.append(predicted_captions)\n","      res_captions.append(sample['raw_captions'][0])\n","\n","  total_scores = total_scores/len(dataloader)\n","    \n","  res_score = {\n","      'bert_score' : total_scores[0],\n","      'bleu_score' : {\n","          'BLEU1' : total_scores[1],\n","          'BLEU2' : total_scores[2],\n","          'BLEU3' : total_scores[3],\n","          'BLEU4' : total_scores[4],\n","      },\n","      'rouge_score' : total_scores[5],\n","      'meteor_score' : total_scores[6],\n","      'cider_score' : total_scores[7]\n","  }\n","\n","  return res_ids, res_predictions"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:45:13.392414Z","iopub.status.busy":"2024-05-07T10:45:13.391517Z","iopub.status.idle":"2024-05-07T10:45:51.511207Z","shell.execute_reply":"2024-05-07T10:45:51.509593Z","shell.execute_reply.started":"2024-05-07T10:45:13.392380Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a776e662ccfc4394ad68201fca2ad380","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9972 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Predicted Concepts: Plain x-ray\n","Prediction : ['Postoperative radiographs Postoperative radiographs showing total hip replacement Postoperative radiographs Postoperative radiographs Postoperative radiographs Postoperative radiographs Postoperative radiographs Postoperative radiographs']\n","Ground_truth : ['Anteroposterior radiograph of the pelvis where eccentric placement of the metal head can be seen. Signs of failure of the greater trochanter synthesis were also present.']\n","       \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"377f3f77b07744a78d0072bf53958514","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c74147357e6f46dc96ac3c52a663198a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68a72ba53e304a908949a8f9d9b2bdd5","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e848ce4accb425e9dcde438b9c9c5db","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e835b6adc07442d98db54c2b94bc3a69","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/3.04G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Predicted Concepts: X-Ray Computed Tomography\n","Prediction : ['CT scan showing right sided obstructive lesion (red arrow) CT scan CT scan CT scan CT scan CT scan CT scan CT scan CT scan CT scan CT scan CT scan']\n","Ground_truth : ['Axial contrasted CT image of larynx, showing left sided glottic versus supraglottic mass.']\n","       \n","Predicted Concepts: Magnetic Resonance Imaging\n","Prediction : ['MRI brain showing asymmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical symmetrical']\n","Ground_truth : ['Annotated MRI coronal view of T2 3D-DESS WE at the level of an erupted third molar region, (point B) notice the intra-oral separator in black giving clear demarcation of the tongue and lingual mucosa of the mandible. Structures identified—lingual nerve (yellow arrow), lingual gingiva (red arrow), lateral border of tongue (pink arrow) and mylohyoid muscle (white arrow)']\n","       \n","Predicted Concepts: X-Ray Computed Tomography\n","Prediction : ['CT scan abdomen showing adnexal mass arrow indicates adnexal mass arrow indicates adnexal mass arrow indicates adnexal mass arrow indicates ad']\n","Ground_truth : ['A 54-year-old female with stratification of bile in the lumen. A nonenhanced CT showed stratification of bile (arrow) in the gallbladder. The patient underwent PC the next day, and the bile was black and viscous']\n","       \n","Predicted Concepts: Chest, Plain x-ray\n","Prediction : ['Chest X-ray showing opacity (arrow) arrowhead indicates opacity X-ray Chest X-ray Chest X-ray Chest X-ray']\n","Ground_truth : ['Chest X-ray face (solitary pulmonary nodule of the heart-phrenic angle).']\n","       \n","Predicted Concepts: X-Ray Computed Tomography\n","Prediction : ['CT abdomen pelvis showing perihepatic hematoma CT abdomen pelvis showing perihepatic hematoma CT abdomen pelvis CT abdomen pelvis showing perihepatic hematoma CT']\n","Ground_truth : ['CT abdomen: the encircled area highlights the gallbladder, which has an asymmetrically thickened wall with scattered radiolucencies']\n","       \n","Predicted Concepts: Abdomen, Plain x-ray\n","Prediction : ['Abdominal X-ray showing bowel loops Abdominal X-ray showing bowel loops Abdominal X-ray Abdominal X-ray showing bowel loops Ab']\n","Ground_truth : ['Plain abdominal radiograph showing signs of small bowel obstruction with dilated small bowel loops']\n","       \n","Predicted Concepts: X-Ray Computed Tomography\n","Prediction : ['CT scan showing mass left lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral aspect right lateral']\n","Ground_truth : ['Chest CT showing an inhomogeneous alteration of the left thyroid lobe, corresponding to the metastatic thyroid involvement from the ACC.']\n","       \n","Predicted Concepts: Ultrasonography\n","Prediction : ['Transvaginal ultrasound image showing apical ballooning (arrow) apical ballooning (arrowhead) apical ballooning (arrowhead) apical ballooning (arrowhead)']\n","Ground_truth : ['Transvaginal ultrasound shows a ureterocele (yellow arrow). Color Doppler demonstrates the flow of urine (white arrow) toward the urinary bladder (UB).']\n","       \n","Predicted Concepts: Magnetic Resonance Imaging\n","Prediction : ['MRI brain showing enhancing lesion extending right lateral tibial artery MRI: magnetic resonance imaging MRI: magnetic resonance imaging MRI: magnetic resonance imaging MRI: magnetic resonance imaging MRI:']\n","Ground_truth : ['Orbital adnexal tumor (yellow arrow) that infiltrated orbital muscles and sheathed the optic nerve.']\n","       \n","Predicted Concepts: Chest, Plain x-ray\n","Prediction : ['Chest X-ray showing bilateral pulmonary opacities Chest X-ray showing bilateral pulmonary opacities Chest X-ray showing bilateral pulmonary opacities Chest']\n","Ground_truth : ['The chest X-ray in the posteroanterior view shows viral pneumonia.']\n","       \n","Predicted Concepts: X-Ray Computed Tomography\n","Prediction : ['CT scan chest showing pulmonary emphysema CT scan chest CT scan chest CT scan chest CT scan chest CT scan chest CT scan chest CT scan chest CT scan chest showing pulmonary e']\n","Ground_truth : ['Bilateral axillary lymphadenopathy (which could be of reactive or neoplastic etiology)']\n","       \n","Predicted Concepts: Magnetic Resonance Imaging\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m convert_logits_concepts \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_id_name\n\u001b[0;32m----> 3\u001b[0m scores, ids, captions, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_caption\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_caption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmodel_concepts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_concepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconvert_logits_concepts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconvert_logits_concepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m: ids, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaption\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions})\n\u001b[1;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n","Cell \u001b[0;32mIn[40], line 17\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model_caption, model_concepts, dataloader, tokenizer, convert_logits_concepts, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m pixel_value \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m image_id \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m predicted_captions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_caption\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_caption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmodel_concepts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_concepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpixel_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpixel_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mconvert_logits_concepts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconvert_logits_concepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m total_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m compute_metrics([predicted_captions], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_captions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m res_ids\u001b[38;5;241m.\u001b[39mappend(image_id[\u001b[38;5;241m0\u001b[39m])\n","Cell \u001b[0;32mIn[39], line 52\u001b[0m, in \u001b[0;36mpredict_caption\u001b[0;34m(model_caption, model_concepts, pixel_value, tokenizer, convert_logits_concepts, device, max_length)\u001b[0m\n\u001b[1;32m     48\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([model_caption\u001b[38;5;241m.\u001b[39mcaptions_decoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id])\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Decoder Start Token\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_length:\n\u001b[0;32m---> 52\u001b[0m   decoded_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_caption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptions_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcross_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state \u001b[38;5;66;03m# batch_size, answer_length, 768\u001b[39;00m\n\u001b[1;32m     57\u001b[0m   output_tokens \u001b[38;5;241m=\u001b[39m model_caption\u001b[38;5;241m.\u001b[39mlm_head(decoded_tokens) \u001b[38;5;66;03m# batch_size, answer_length, vocab_size\u001b[39;00m\n\u001b[1;32m     59\u001b[0m   tokens_ids \u001b[38;5;241m=\u001b[39m output_tokens\u001b[38;5;241m.\u001b[39margmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# batch_size, answer_length\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1109\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1095\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1096\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         output_attentions,\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:719\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:630\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    619\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    628\u001b[0m ):\n\u001b[1;32m    629\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 630\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    642\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:521\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    518\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    519\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    520\u001b[0m )\n\u001b[0;32m--> 521\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[1;32m    526\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[1;32m    527\u001b[0m     query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    528\u001b[0m )  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:496\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    492\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(proj_layer(hidden_states))\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(\u001b[43mproj_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, key_length, dim_per_head)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["convert_logits_concepts = data.get_id_name\n","\n","scores, ids, captions, predictions = predict(model_caption = model_caption,\n","                           model_concepts = model_concepts,\n","                          dataloader = dataloader,\n","                          tokenizer = tokenizer,\n","                          convert_logits_concepts = convert_logits_concepts,\n","                          device = device)\n","\n","df = pd.DataFrame({'ID': ids, 'Caption': predictions})\n","df.to_csv('concept_val_run.csv')\n","torch.save(scores, 'concept_val_scores.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4956888,"sourceId":8344765,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
